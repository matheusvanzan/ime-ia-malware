'''

Parse all docs inside given directory
Read each line and remove junk chars


'''

import os
import pandas as pd

from collections import Counter

class DocManager:

    def __init__(self, docs_limit, path_data_raw, path_data_tokens, \
        chars_to_remove, tokenizer):

        self.docs_limit = docs_limit
        self.path_data_raw = path_data_raw
        self.path_data_tokens = path_data_tokens
        self.chars_to_remove = chars_to_remove
        self.tokenizer = tokenizer

    def create_df_from_tokens(self, tokens, columns):
        df = pd.DataFrame(Counter(tokens).items(), columns = columns)
        return df

    def create_token_docs(self, filetype='.asm'):

        list_dir = os.listdir(self.path_data_raw)
        total = len(list_dir) if len(list_dir) < self.docs_limit else self.docs_limit

        for i, filename in enumerate(list_dir):

            if i+1 > self.docs_limit:
                break

            filepath = os.path.join(self.path_data_raw, filename)
            type_ = os.path.splitext(filepath)[-1]
            if type_ != filetype:
                break            
            
            with open(filepath, encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            for char_to_remove in self.chars_to_remove:
                content = content.replace(char_to_remove, ' ')

            content = ' '.join(content.split()) # remove multiple whitespaces

            print(f'{i+1} of {total} - {filename}')
            for w in range(self.tokenizer.token_max_len):
                window = w+1

                # TODO: should it be parameter?
                token_min_len = 2
                if window == 1:
                    token_min_len = 1

                tokens = self.tokenizer.build_tokenizer(
                    content = content,
                    window = window,
                    token_min_len = token_min_len
                )
                print(f'    window = {window}: found {len(tokens)} tokens')

                # create pd df
                filedir = os.path.join(self.path_data_tokens, str(window))
                if not os.path.exists(filedir):
                    os.mkdir(filedir)

                filename = filename.replace(type_, '')
                filename_tokens = f'{filename}.csv'
                filepath_tokens = os.path.join(filedir, filename_tokens)
                df = self.create_df_from_tokens(
                    tokens = tokens, 
                    columns = ['token', 'count']
                )
                # print(df)
                df.to_csv(filepath_tokens, index=False, encoding='utf-8')

    def df_2_by_2_T(df):
        return df

    def load_token_docs(self, window):
        filedir = os.path.join(self.path_data_tokens, str(window))

        df_list = []
        for w in range(window):
            window = w+1
            print(f'window = {window}')

            list_dir = os.listdir(filedir)
            total = len(list_dir) if len(list_dir) < self.docs_limit else self.docs_limit

            # df = []
            for i, filename in enumerate(list_dir):
                print(f'    {i}, {filename}')
                filepath = os.path.join(filedir, filename)
                df_single = pd.read_csv(filepath, encoding='utf-8')
                df_single.set_index('token')
                print(df_single)
            # df_list.append(df)

        return df_list

        


def main():
    pass


if __name__ == '__main__':
    main()