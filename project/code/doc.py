'''

Parse all docs inside given directory
Read each line and remove junk chars


'''

import os
import pickle
import pandas as pd

from functools import reduce

from sklearn.feature_extraction.text import CountVectorizer

class DocManager:

    def __init__(self, docs_limit, path_data_raw, path_data_counts, \
        chars_to_remove, token_max_len, max_features):

        self.docs_limit = docs_limit
        self.path_data_raw = path_data_raw
        self.path_data_counts = path_data_counts
        self.chars_to_remove = chars_to_remove
        self.token_max_len = token_max_len
        self.max_features = max_features

    def preprocessor(self, content):
        for char_to_remove in self.chars_to_remove:
            content = content.replace(char_to_remove, ' ')

        content = ' '.join(content.split()) # remove multiple whitespaces

        return content


    def create_df_from_asm(self, filetype='.asm'):
        list_dir = os.listdir(self.path_data_raw)
        total = len(list_dir) if len(list_dir) < self.docs_limit else self.docs_limit

        for i, filename in enumerate(list_dir):

            if i+1 > self.docs_limit:
                break

            filepath = os.path.join(self.path_data_raw, filename)
            type_ = os.path.splitext(filepath)[-1]
            if type_ != filetype:
                break            
            
            with open(filepath, encoding='utf-8', errors='ignore') as f:
                content = f.read()

            filename = filename.replace(type_, '')

            print(f'{i+1} of {total} - {filename}')
            df = self.create_df_from_single_asm(
                filename = filename, 
                content = content
            )

    def create_df_from_single_asm(self, filename, content):       

        for ngram in range(1, self.token_max_len+1):
            vectorizer = CountVectorizer(
                preprocessor = self.preprocessor,
                ngram_range = (ngram, ngram),
                max_features = self.max_features

            )
            X = vectorizer.fit_transform([content])
            features = vectorizer.get_feature_names_out()
            df = pd.DataFrame(
                data = X.toarray(),
                index = [filename],
                columns = features
            )
            print(df)

            path = os.path.join(self.path_data_counts, f'{ngram}')
            if not os.path.exists(path):
                os.mkdir(path)

            df.to_pickle(os.path.join(path, f'{filename}.pkl'))

                


    def load_df_from_pickle(self, ngram = None):

        # if not ngram:
        #     ngram = [n for n in range(1, self.token_max_len)]

        path = os.path.join(self.path_data_counts, f'{ngram}')
        list_dir = os.listdir(path)
        total = len(list_dir) if len(list_dir) < self.docs_limit else self.docs_limit

        df_list = []
        for i, filename in enumerate(list_dir):

            if i+1 > self.docs_limit:
                break

            print(f'{i+1} of {total} - {filename}')

            filepath = os.path.join(path, filename)
            df = pd.read_pickle(filepath)
            print(df)
            df_list.append(df)

        df_final = pd.concat(df_list)
        df_final.fillna(0, inplace=True)
        return df_final

        


def main():
    pass


if __name__ == '__main__':
    main()