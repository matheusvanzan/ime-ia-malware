'''

Parse all docs inside given directory
Read each line and remove junk chars


'''

import os
import pickle
import pandas as pd

from functools import reduce

from sklearn.feature_extraction.text import CountVectorizer

from concurrent.futures import ProcessPoolExecutor


def create_chunks(lst, n):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]



class DocManager:

    def __init__(self, docs_limit, path_data_raw, path_data_counts, \
        chars_to_remove, max_features, ngram, max_workers):

        self.docs_limit = docs_limit
        self.path_data_raw = path_data_raw
        self.path_data_counts = path_data_counts
        self.chars_to_remove = chars_to_remove
        self.max_features = max_features
        self.ngram = ngram
        self.max_workers = max_workers

    def preprocessor(self, content):
        for char_to_remove in self.chars_to_remove:
            content = content.replace(char_to_remove, ' ')

        content = ' '.join(content.split()) # remove multiple whitespaces

        return content

    def create_df_from_asm(self, filetype='.asm'):
        list_dir = os.listdir(self.path_data_raw)
        total = len(list_dir) if len(list_dir) < self.docs_limit else self.docs_limit

        values = []
        for i, filename in enumerate(list_dir):

            if i+1 > self.docs_limit:
                break

            filepath = os.path.join(self.path_data_raw, filename)
            type_ = os.path.splitext(filepath)[-1]
            if type_ != filetype:
                break            
            
            with open(filepath, encoding='utf-8', errors='ignore') as f:
                content = f.read()

            filename = filename.replace(type_, '')

            values.append((i+1, total, filename, content))


        # Singleprocessor
        # for value in values:
        #     self.create_df_from_single_asm(value)

        # Multiprocessor
        with ProcessPoolExecutor(max_workers = self.max_workers) as executor:
            for i, df in enumerate(executor.map( \
                    self.create_df_from_single_asm, values)):
                print(f'Finish {i+1} of {total} - {df.shape}')

    def create_df_from_single_asm(self, values):

        i, total, filename, content = values
        print(f'Start {i} of {total} - {filename}')     

        vectorizer = CountVectorizer(
            preprocessor = self.preprocessor,
            ngram_range = (self.ngram, self.ngram),
            max_features = self.max_features

        )
        X = vectorizer.fit_transform([content])
        # vectorizer.fit([content])
        features = vectorizer.get_feature_names_out()
        df = pd.DataFrame(
            data = X.toarray(),
            index = [filename],
            columns = features
        )
        # print(df)

        path = os.path.join(self.path_data_counts, f'{self.ngram}')
        if not os.path.exists(path):
            os.mkdir(path)

        df.to_pickle(os.path.join(path, f'{filename}.pkl'))
        # with open(os.path.join(path, f'{filename}.pkl'), 'wb') as f:
        #     pickle.dump(vectorizer, f)

        return df


    def load_df_from_pickle(self):

        # if not ngram:
        #     ngram = [n for n in range(1, self.token_max_len)]

        path = os.path.join(self.path_data_counts, f'{self.ngram}')
        list_dir = os.listdir(path)
        total = len(list_dir) if len(list_dir) < self.docs_limit else self.docs_limit

        df_list = []
        for i, filename in enumerate(list_dir):
            if i+1 > self.docs_limit:
                break
            print(f'{i+1} of {total} - {filename}')

            filepath = os.path.join(path, filename)
            df = pd.read_pickle(filepath)
            print(df)
            df_list.append(df)


        # Singleprocessor
        # return self.concat_df_list(df_list)

        # Multiprocessor
        chunks = list(create_chunks(df_list, 100))
        df_chunk_list = []
        with ProcessPoolExecutor(max_workers = self.max_workers) as executor:
            for i, df in enumerate(executor.map( \
                    self.concat_df_list, chunks)):
                print(f'finish chunk {i} of {len(chunks)} - {df.shape}')
                df_chunk_list.append(df)
        return self.concat_df_list(df_chunk_list)
    
    def concat_df_list(self, df_list):
        print('start concat')
        df_final = pd.concat(df_list)
        df_final.fillna(0, inplace=True)
        s = df_final.sum()
        df_final = df_final[s.sort_values(ascending=False).index[:self.max_features]]
        return df_final



def main():
    pass


if __name__ == '__main__':
    main()