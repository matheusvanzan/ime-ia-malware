'''

Extract Tokens from a single document

Tokens can have multiple dimentions (1, 2, 3, ...)


'''

class Tokenizer:
    #
    # remember not to remove repeated tokens
    # they set the count and frequency in the Vectorizer
    #

    def __init__(self, token_max_len):
        self.token_max_len = token_max_len

    def build_tokenizer(self, content, window=1, token_min_len=2):
        
        tokens = []
        words = content.split(' ')
        for i in range(len(words) - window + 1):
            token_words = \
                [words[i+j] for j in range(window) \
                    if len(words[i+j]) >= token_min_len]
            
            if len(token_words) == window:
                tokens.append(' '.join(token_words))

        return tokens

    def build_df(self, window_list):
        '''
            returns pd df like
            1   a
            1   b
            2   a b
        '''
        pass




def main():
    pass


if __name__ == '__main__':
    main()