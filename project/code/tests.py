'''

tests for tokens module

'''
import unittest

import pandas as pd

from tokenizer import Tokenizer
from doc import DocManager

class TokenTest(unittest.TestCase):

    def test_build_tokenizer(self):
        content = 'a b c'
        tokens_1 = ['a', 'b', 'c']
        tokens_2 = ['a b', 'b c']

        tokenizer = Tokenizer()

        self.assertEqual(
            tokenizer.build_tokenizer(
                content = content,
                window = 1,
                token_min_len = 1
            ), tokens_1
        )

        self.assertEqual(
            tokenizer.build_tokenizer(
                content = content,
                window = 2,
                token_min_len = 1
            ), tokens_2
        )


class TokenTest(unittest.TestCase):
        
    def test_create_df_from_tokens(self):
        doc_manager = DocManager(
            path_data_raw = '',
            path_data_tokens = '',
            chars_to_remove = '',
            tokenizer = None
        )

        columns = ['token', 'count']
        tokens = list('aaabbccccc')
        data = [('a', 3), ('b', 2), ('c', 5)]
        df = pd.DataFrame(data, columns = columns)
        
        self.assertEqual(True,
            df.equals(doc_manager.create_df_from_tokens(
                tokens = tokens,
                columns = columns
            ))
        )

def main():
    unittest.main()

if __name__ == '__main__':
    main()