'''

tests for tokens module

'''
import unittest

import pandas as pd

from tokenizer import Tokenizer
from doc import DocManager


import settings

class DocTest(unittest.TestCase):

    def __init__(self, *args, **kwargs):
        super(DocTest, self).__init__(*args, **kwargs)
        self.filename = 'data/test.asm'
        self.content = '''
            a-bb ccc+dd
        '''

        self.content_processed = 'a bb ccc dd'
        self.columns = 'i,bb,bb ccc,bb ccc dd,ccc,ccc dd,dd'.split(',')
        self.data = [
            [self.filename] + [1]*(len(self.columns)-1)
        ]

        self.doc_manager = DocManager(
            docs_limit = 10,
            path_data_raw = '',
            path_data_counts = '',
            chars_to_remove = settings.NPL_CHARS_TO_REMOVE,
            token_max_len = settings.NPL_TOKEN_MAX_LEN
        )

    def test_preprocessor(self):
        self.assertEqual(
            self.doc_manager.preprocessor(self.content),
            self.content_processed
        )

    def test_create_df_from_doc(self):
        df_code = self.doc_manager.create_df_from_single_asm(
            filename = self.filename,
            content = self.content
        )
        df_test = pd.DataFrame(self.data, columns = self.columns)
        df_test.set_index('i', inplace=True)
        self.assertEqual(True, df_code.equals(df_test))
        



def main():
    unittest.main()

if __name__ == '__main__':
    main()